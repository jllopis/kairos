# Gu√≠a de Observabilidad

Kairos integra **OpenTelemetry (OTEL)** desde el inicio, proporcionando visibilidad completa en:

- **Errores**: Clasificaci√≥n por tipo, componente, recoverabilidad
- **Resiliencia**: Reintentos, circuit breaker, fallbacks, timeouts
- **Salud**: Estado de componentes, degradaci√≥n, recuperaci√≥n
- **Rendimiento**: Tasa de errores, latencia de recuperaci√≥n

## Table of Contents

1. [Introducci√≥n](#introduccion)
2. [Arquitectura de Observabilidad](#arquitectura-de-observabilidad)
3. [Atributos de Span Enriquecidos](#atributos-de-span-enriquecidos)
4. [M√©tricas Disponibles](#metricas-disponibles)
5. [Dashboards](#dashboards)
6. [Reglas de Alerta](#reglas-de-alerta)
7. [Ejemplos de Uso](#ejemplos-de-uso)
8. [Integraci√≥n con Backends](#integracion-con-backends)
9. [SLOs y Recomendaciones](#slos-y-recomendaciones)

---

## Introducci√≥n

Kairos integra **OpenTelemetry (OTEL)** desde el inicio, proporcionando visibilidad completa en:

- **Errores**: Clasificaci√≥n por tipo, componente, recoverabilidad
- **Resiliencia**: Reintentos, circuit breaker, fallbacks, timeouts
- **Salud**: Estado de componentes, degradaci√≥n, recuperaci√≥n
- **Rendimiento**: Tasa de errores, latencia de recuperaci√≥n

### ¬øPor qu√© esto importa?

Sin observabilidad, no sabes si tus patrones de resiliencia funcionan. Con Kairos:

- ‚úÖ **Visibilidad Real**: Cada error se registra con contexto completo
- ‚úÖ **Detecci√≥n Temprana**: Alertas antes de que se propague el problema
- ‚úÖ **Debugging R√°pido**: Dashboards con correlaciones error ‚Üî recuperaci√≥n
- ‚úÖ **SLOs Medibles**: Datos para definir y cumplir SLOs de confiabilidad

---

## Arquitectura de Observabilidad

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Aplicaci√≥n Kairos                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  pkg/errors ‚Üí KairosError (typed errors)                   ‚îÇ
‚îÇ  pkg/resilience ‚Üí Retry, CircuitBreaker, Fallback, etc.    ‚îÇ
‚îÇ  pkg/telemetry ‚Üí RecordError(), RecordErrorMetric()        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  OTEL SDK: Metrics + Traces                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    OTEL Exporters                           ‚îÇ
‚îÇ  ‚îú‚îÄ Stdout (desarrollo)                                     ‚îÇ
‚îÇ  ‚îú‚îÄ gRPC/OTLP (producci√≥n: Datadog, New Relic, etc.)       ‚îÇ
‚îÇ  ‚îî‚îÄ Jaeger, Prometheus, etc.                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ              Backends & Visualizaci√≥n                       ‚îÇ
‚îÇ  ‚îú‚îÄ Prometheus (m√©tricas + alertas)                        ‚îÇ
‚îÇ  ‚îú‚îÄ Grafana (dashboards)                                    ‚îÇ
‚îÇ  ‚îî‚îÄ AlertManager (notificaciones)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de Datos

```
Error ocurre
    ‚Üì
KairosError creado con c√≥digo + contexto
    ‚Üì
telemetry.RecordError(span, err) ‚Üí Atributos en trace
    ‚Üì
metrics.RecordErrorMetric(ctx, err, "component") ‚Üí Contador OTEL
    ‚Üì
OTEL Exporter env√≠a a backend (OTLP, Prometheus, etc.)
    ‚Üì
Dashboards visualizan ‚Üí Alertas disparan si es necesario
    ‚Üì
Equipo responde basado en datos
```

---

## Atributos de Span Enriquecidos

Kairos a√±ade atributos sem√°nticos ricos a los spans de OTEL para facilitar debugging y an√°lisis.

### Spans Disponibles

| Span | Descripci√≥n |
|------|-------------|
| `Agent.Run` | Ejecuci√≥n completa del agente |
| `Agent.LLM.Chat` | Llamada al LLM |
| `Agent.Tool.Call` | Ejecuci√≥n de una tool |

### Atributos del Agente (`Agent.Run`)

| Atributo | Tipo | Descripci√≥n |
|----------|------|-------------|
| `kairos.agent.id` | string | Identificador del agente |
| `kairos.agent.role` | string | Rol del agente |
| `kairos.agent.model` | string | Modelo LLM usado |
| `kairos.agent.run_id` | string | ID √∫nico de la ejecuci√≥n |
| `kairos.agent.max_iterations` | int | L√≠mite de iteraciones |
| `kairos.session.id` | string | ID de sesi√≥n (si hay conversaci√≥n) |
| `kairos.conversation.enabled` | bool | Si hay memoria de conversaci√≥n |
| `kairos.conversation.message_count` | int | Mensajes en historial |
| `kairos.tools.count` | int | Total de tools disponibles |
| `kairos.tools.local_count` | int | Tools locales |
| `kairos.tools.mcp_count` | int | Tools MCP |
| `kairos.tools.skill_count` | int | Skills como tools |
| `kairos.tools.names` | []string | Nombres de tools |
| `kairos.memory.enabled` | bool | Si hay memoria sem√°ntica |
| `kairos.memory.type` | string | Tipo de memoria |
| `kairos.task.id` | string | ID de task (si existe) |
| `kairos.task.goal` | string | Objetivo de la task |
| `kairos.task.status` | string | Estado de la task |

### Atributos del LLM (`Agent.LLM.Chat`)

| Atributo | Tipo | Descripci√≥n |
|----------|------|-------------|
| `gen_ai.request.model` | string | Modelo solicitado |
| `gen_ai.system` | string | Provider (openai, anthropic...) |
| `gen_ai.request.messages` | int | N√∫mero de mensajes enviados |
| `gen_ai.tool_calls` | int | Tool calls en la respuesta |
| `gen_ai.usage.input_tokens` | int | Tokens de entrada |
| `gen_ai.usage.output_tokens` | int | Tokens de salida |
| `gen_ai.duration_ms` | float | Duraci√≥n en ms |
| `gen_ai.finish_reason` | string | Raz√≥n de finalizaci√≥n |

### Atributos de Tool (`Agent.Tool.Call`)

| Atributo | Tipo | Descripci√≥n |
|----------|------|-------------|
| `kairos.tool.name` | string | Nombre de la tool |
| `kairos.tool.call_id` | string | ID de la llamada |
| `kairos.tool.source` | string | Origen: "local", "mcp", "skill" |
| `kairos.tool.duration_ms` | float | Duraci√≥n en ms |
| `kairos.tool.success` | bool | Si tuvo √©xito |
| `kairos.tool.arguments` | string | Argumentos (truncados) |
| `kairos.tool.result` | string | Resultado (truncado) |

### Ejemplo en Jaeger

```
Agent.Run (350ms)
‚îú‚îÄ‚îÄ kairos.agent.id: "assistant"
‚îú‚îÄ‚îÄ kairos.agent.model: "gpt-4"
‚îú‚îÄ‚îÄ kairos.tools.count: 3
‚îú‚îÄ‚îÄ kairos.conversation.enabled: true
‚îú‚îÄ‚îÄ kairos.session.id: "user-123"
‚îÇ
‚îú‚îÄ‚îÄ Agent.LLM.Chat (200ms)
‚îÇ   ‚îú‚îÄ‚îÄ gen_ai.request.model: "gpt-4"
‚îÇ   ‚îú‚îÄ‚îÄ gen_ai.request.messages: 5
‚îÇ   ‚îî‚îÄ‚îÄ gen_ai.tool_calls: 1
‚îÇ
‚îî‚îÄ‚îÄ Agent.Tool.Call (100ms)
    ‚îú‚îÄ‚îÄ kairos.tool.name: "search"
    ‚îú‚îÄ‚îÄ kairos.tool.source: "mcp"
    ‚îú‚îÄ‚îÄ kairos.tool.success: true
    ‚îî‚îÄ‚îÄ kairos.tool.duration_ms: 98.5
```

---

## M√©tricas Disponibles

Kairos expone **5 m√©tricas principales** v√≠a OTEL:

### 1. `kairos.errors.total` (Counter)

**Descripci√≥n**: N√∫mero total de errores por c√≥digo de error y componente.

**Atributos**:
- `error.code`: C√≥digo del error (TOOL_FAILURE, TIMEOUT, LLM_ERROR, etc.)
- `component`: Nombre del componente que reporta el error
- `recoverable`: "true" o "false"

**Ejemplo**:
```
kairos.errors.total{error.code="TIMEOUT", component="llm-service", recoverable="true"} = 42
kairos.errors.total{error.code="TOOL_FAILURE", component="executor", recoverable="false"} = 3
```

**Uso**: Medir tasa de errores, identificar componentes problem√°ticos, alertar sobre no-recuperables.

---

### 2. `kairos.errors.recovered` (Counter)

**Descripci√≥n**: N√∫mero de errores recuperados exitosamente (reintentos que funcionaron, fallbacks usados).

**Atributos**:
- `error.code`: C√≥digo del error recuperado

**Ejemplo**:
```
kairos.errors.recovered{error.code="TIMEOUT"} = 38  (de 42 totales)
kairos.errors.recovered{error.code="TOOL_FAILURE"} = 0  (de 3 no-recuperables)
```

**Uso**: Calcular **tasa de recuperaci√≥n** = `errors.recovered / errors.total` (objetivo: >80%)

---

### 3. `kairos.errors.rate` (Gauge)

**Descripci√≥n**: Tasa de errores por minuto por componente.

**Atributos**:
- `component`: Nombre del componente

**Ejemplo**:
```
kairos.errors.rate{component="llm-service"} = 2.5    (2.5 errores/min)
kairos.errors.rate{component="executor"} = 0.1       (0.1 errores/min)
```

**Uso**: Umbral para alertas. Valores de referencia:
- Normal: < 1 error/min
- Advertencia: 1-5 errores/min
- Cr√≠tico: > 5 errores/min

---

### 4. `kairos.health.status` (Gauge)

**Descripci√≥n**: Estado de salud del componente en el momento de la medici√≥n.

**Valores**:
- `2` = HEALTHY (operativo normalmente)
- `1` = DEGRADED (funciona pero con limitaciones)
- `0` = UNHEALTHY (no funciona, usando fallback)

**Atributos**:
- `component`: Nombre del componente

**Ejemplo**:
```
kairos.health.status{component="llm-service"} = 2      (verde)
kairos.health.status{component="cache"} = 1            (amarillo)
kairos.health.status{component="external-api"} = 0     (rojo)
```

**Uso**: 
- Dashboards: Grid de colores (rojo/amarillo/verde)
- Routing: Desviar tr√°fico de componentes no-saludables
- Alertas: UNHEALTHY ‚Üí investigaci√≥n inmediata

---

### 5. `kairos.circuitbreaker.state` (Gauge)

**Descripci√≥n**: Estado actual del circuit breaker para cada componente.

**Valores**:
- `2` = CLOSED (operando normalmente, solicitudes fluyendo)
- `1` = HALF_OPEN (probando recuperaci√≥n, solicitudes limitadas)
- `0` = OPEN (circuito roto, fallback activo, solicitudes rechazadas)

**Atributos**:
- `component`: Nombre del componente

**Ejemplo**:
```
kairos.circuitbreaker.state{component="api-client"} = 2         (cerrado)
kairos.circuitbreaker.state{component="external-service"} = 1   (medio-abierto)
kairos.circuitbreaker.state{component="failing-dep"} = 0        (abierto)
```

**Uso**:
- Entender cascadas de fallos
- Identificar qu√© servicios est√°n afectando a otros
- Medir tiempo de recuperaci√≥n

---

## Dashboards

### Dashboard 1: Error Rate & Recovery (Tasa de Errores y Recuperaci√≥n)

**Prop√≥sito**: Entender la salud general del sistema en t√©rminos de errores y resiliencia.

#### Panel 1.1: Error Rate por C√≥digo (√öltimas 24h)

**Query (PromQL)**:
```promql
rate(kairos.errors.total{error_code=~".+"}[5m])
```

**Configuraci√≥n Grafana**:
- **Tipo**: Line Chart
- **Eje X**: Tiempo (5m bucket)
- **Eje Y**: Errores por segundo
- **Leyenda**: Por `error_code` (TOOL_FAILURE, TIMEOUT, LLM_ERROR, etc.)
- **Colores**: Rojo para cr√≠ticos (CodeInternal, CodeMemoryError), naranja para recuperables

**Interpretaci√≥n**:
- L√≠nea suave y baja: Sistema sano ‚úÖ
- Picos ocasionales: Dentro de lo normal si se recupera
- L√≠nea constantemente alta: **Alerta** ‚Üí investigar causa ra√≠z

---

#### Panel 1.2: Tasa de Recuperaci√≥n (%)

**Query (PromQL)**:
```promql
(
  rate(kairos.errors.recovered[5m]) 
  / 
  rate(kairos.errors.total[5m])
) * 100
```

**Configuraci√≥n Grafana**:
- **Tipo**: Gauge
- **Umbral Inferior**: 80% (amarillo)
- **Umbral Superior**: 90% (verde)
- **Unidad**: percent

**Interpretaci√≥n**:
- Verde (>90%): Excelente resiliencia ‚úÖ
- Amarillo (80-90%): Aceptable, monitorear
- Rojo (<80%): **Problema** ‚Üí revisar configuraci√≥n de reintentos/fallbacks

---

#### Panel 1.3: Error Rate por Componente (Gauge)

**Query (PromQL)**:
```promql
sum(rate(kairos.errors.total[5m])) by (component)
```

**Configuraci√≥n Grafana**:
- **Tipo**: Table (mostrar top 5-10)
- **Columnas**: component, error_rate
- **Ordenar por**: error_rate DESC
- **Colores**: 
  - Rojo: > 5 errores/min
  - Naranja: 1-5 errores/min
  - Verde: < 1 error/min

**Interpretaci√≥n**:
- Identifica qu√© componentes generan m√°s errores
- Ejemplo: si `llm-service` est√° en rojo ‚Üí investigar calidad del modelo o sobrecarga

---

### Dashboard 2: Component Health (Salud de Componentes)

**Prop√≥sito**: Monitoreo en tiempo real del estado operativo de cada componente.

#### Panel 2.1: Health Status Grid

**Query (PromQL)**:
```promql
kairos.health.status{component=~".+"}
```

**Configuraci√≥n Grafana**:
- **Tipo**: Stat or Status Grid
- **Mostrar**: Valor actual (0, 1, 2)
- **Color Mapping**:
  - 0 ‚Üí Rojo (UNHEALTHY)
  - 1 ‚Üí Amarillo (DEGRADED)
  - 2 ‚Üí Verde (HEALTHY)
- **Layout**: Grid (4-5 columnas)

**Componentes t√≠picos a monitorear**:
- llm-service
- cache
- database
- external-api
- tool-executor
- memory

**Interpretaci√≥n**:
- **Verde**: ‚úÖ Todo est√° bien
- **Amarillo**: ‚ö†Ô∏è Observar, puede empeorar
- **Rojo**: üî¥ ACCI√ìN REQUERIDA ‚Üí fallback activo, investigar

---

#### Panel 2.2: Circuit Breaker States

**Query (PromQL)**:
```promql
kairos.circuitbreaker.state{component=~".+"}
```

**Configuraci√≥n Grafana**:
- **Tipo**: Status Panels (uno por componente importante)
- **Color Mapping**:
  - 2 ‚Üí Verde (CLOSED: operando normalmente)
  - 1 ‚Üí Naranja (HALF_OPEN: probando recuperaci√≥n)
  - 0 ‚Üí Rojo (OPEN: fallback activo)
- **Mostrar**: Estado actual + √∫ltima actualizaci√≥n

**Interpretaci√≥n**:
- **CLOSED**: Flujo normal ‚úÖ
- **HALF_OPEN**: En recuperaci√≥n, monitorear pr√≥ximos minutos
- **OPEN**: Problema cr√≠tico, usar fallback, investigar dependencia

---

#### Panel 2.3: Health Timeline (√öltimas 24h)

**Query (PromQL)**:
```promql
changes(kairos.health.status{component=~".+"}[1h])
```

**Configuraci√≥n Grafana**:
- **Tipo**: Time Series Heatmap
- **Eje X**: Tiempo (1h bucket)
- **Eje Y**: Componente
- **Colores**: Intensidad = frecuencia de cambios

**Interpretaci√≥n**:
- L√≠neas suave: Componente estable ‚úÖ
- Muchas l√≠neas: Componente inestable ‚ö†Ô∏è
- Largo per√≠odo rojo: Outage üî¥

---

### Dashboard 3: Error Details (Detalles de Errores)

**Prop√≥sito**: Deep dive en patrones espec√≠ficos de errores.

#### Panel 3.1: Error Breakdown Table

**Query (PromQL)**:
```promql
sum(rate(kairos.errors.total[5m])) by (error_code, component, recoverable)
```

**Configuraci√≥n Grafana**:
- **Tipo**: Table
- **Columnas**: error_code, component, recoverable, rate
- **Ordenar por**: rate DESC
- **Filtros**: Permitir drill-down por error_code

**Interpretaci√≥n**:
- ¬øQu√© error es m√°s frecuente? TIMEOUT > TOOL_FAILURE > LLM_ERROR
- ¬øCu√°l es menos recuperable? Buscar non-recoverable
- Ejemplo:
  ```
  TOOL_FAILURE | executor | false | 0.05 errors/sec ‚Üê CR√çTICO
  TIMEOUT      | llm-svc  | true  | 2.50 errors/sec ‚Üê Normal
  ```

---

#### Panel 3.2: Timeout vs Circuit Breaker Correlation

**Query (PromQL)**:
```promql
# Gr√°fico dual axis
Eje 1: rate(kairos.errors.total{error_code="TIMEOUT"}[5m])
Eje 2: kairos.circuitbreaker.state{component=~".+"}
```

**Configuraci√≥n Grafana**:
- **Tipo**: Time Series (dual axis)
- **Eje Izquierdo**: Error rate (l√≠nea roja)
- **Eje Derecho**: Circuit state (l√≠nea azul escalonada)

**Interpretaci√≥n**:
- ¬øLos timeouts causan que circuit breaker abra?
- Ejemplo: L√≠nea roja sube ‚Üí l√≠nea azul pasa de 2‚Üí0 (CLOSED‚ÜíOPEN)
- Esto sugiere que timeouts disparan el circuit breaker (comportamiento esperado)

---

#### Panel 3.3: Recovery Latency

**Query (PromQL)**:
```promql
# Medida: tiempo entre error y recuperaci√≥n exitosa
# (Requiere instrumentaci√≥n manual: timestamp error + timestamp recovery)
histogram_quantile(0.95, rate(kairos.errors.recovered[5m]))
```

**Configuraci√≥n Grafana**:
- **Tipo**: Stat (p95 latencia)
- **Unidad**: ms o s
- **Umbral**: < 5s ideal

**Interpretaci√≥n**:
- p95 < 2s: Excelente resiliencia ‚úÖ
- p95 > 10s: Lento, revisar configuraci√≥n de reintentos

---

## Reglas de Alerta

### Arquitectura de Alertas

```
Prometheus (eval√∫a reglas cada 30s)
    ‚Üì
Regla dispara si condici√≥n es verdadera
    ‚Üì
AlertManager recibe alert
    ‚Üì
Enruta a: Slack, PagerDuty, Email, etc.
    ‚Üì
Equipo act√∫a
```

### Alert 1: High Error Rate (Tasa de Errores Alta)

**Nombre**: `KairosHighErrorRate`

**Regla (Prometheus)**:
```yaml
alert: KairosHighErrorRate
expr: rate(kairos.errors.total[5m]) > 10
for: 2m
severity: critical
annotations:
  summary: "Kairos error rate muy alta"
  description: |
    Tasa de errores: {{ $value }} errors/sec (umbral: 10)
    Componente: {{ $labels.component }}
    C√≥digo: {{ $labels.error_code }}
  action: "Verificar logs del componente, posible sobrecarga o dependencia ca√≠da"
```

**Cu√°ndo dispara**:
- M√°s de 10 errores/segundo durante 2+ minutos consecutivos

**Acciones**:
1. ‚úÖ Verificar logs: `kubectl logs -f <pod>`
2. ‚úÖ Revisar estado de dependencias: ¬øLLM ca√≠do? ¬øBD lenta?
3. ‚úÖ Aumentar recursos si es sobrecarga
4. ‚úÖ Rollback si fue cambio reciente

**Remediaci√≥n**:
```bash
# Escalar servicio
kubectl scale deployment llm-service --replicas=3

# Ver √∫ltimo error
kubectl logs <pod> --tail=100 | grep ERROR

# Reiniciar si es necesario
kubectl rollout restart deployment llm-service
```

---

### Alert 2: Low Recovery Rate (Tasa de Recuperaci√≥n Baja)

**Nombre**: `KairosLowRecoveryRate`

**Regla (Prometheus)**:
```yaml
alert: KairosLowRecoveryRate
expr: |
  (
    rate(kairos.errors.recovered[5m]) 
    / 
    rate(kairos.errors.total[5m])
  ) < 0.8
for: 5m
severity: warning
annotations:
  summary: "Tasa de recuperaci√≥n de errores baja"
  description: |
    Recovery rate: {{ $value | humanizePercentage }} (meta: >= 80%)
    Recuperados: {{ $value_recovered }}
    Totales: {{ $value_total }}
  action: "Revisar configuraci√≥n de reintentos y fallbacks"
```

**Cu√°ndo dispara**:
- Menos del 80% de errores se recuperan durante 5+ minutos

**Acciones**:
1. ‚úÖ Revisar qu√© errores no se recuperan: `kairos.errors.recovered / kairos.errors.total by error_code`
2. ‚úÖ ¬øSon non-recoverable por dise√±o? (p.ej., CodeUnauthorized)
3. ‚úÖ ¬øFallan los reintentos? Aumentar MaxAttempts
4. ‚úÖ ¬øFallback no configurado? Activar fallback strategy

**Remediaci√≥n**:
```go
// Aumentar intentos de reintento
retryConfig := resilience.DefaultRetryConfig().
    WithMaxAttempts(5).  // Era 3
    WithInitialDelay(100 * time.Millisecond)

// Activar fallback
fallback := &resilience.CachedFallback{
    Cache: lastKnownGoodValue,
}
```

---

### Alert 3: Circuit Breaker Open (Circuito Abierto)

**Nombre**: `KairosCircuitBreakerOpen`

**Regla (Prometheus)**:
```yaml
alert: KairosCircuitBreakerOpen
expr: kairos.circuitbreaker.state{component=~".+"} == 0
for: 1m
severity: critical
annotations:
  summary: "Circuit breaker abierto"
  description: |
    Componente: {{ $labels.component }}
    Estado: OPEN (usando fallback)
    Fallback puede ser m√°s lento o degradado
  action: |
    1. Investigar por qu√© el servicio falla
    2. Verificar dependencias (DB, APIs externas)
    3. Una vez estable, circuit breaker se auto-resetea
```

**Cu√°ndo dispara**:
- Circuit breaker abre (demasiados errores consecutivos) por 1+ minuto

**Acciones**:
1. ‚úÖ Identificar causara√≠z: ¬øPor qu√© falla el servicio?
2. ‚úÖ Ver estado de dependencias
3. ‚úÖ Reiniciar servicio si es necesario
4. ‚úÖ Escalar recursos si es sobrecarga

**Remediaci√≥n**:
```bash
# Ver estado de dependencia
curl https://external-api.com/health

# Si la dependencia est√° down, contactar al equipo
# El circuit breaker se auto-recuperar√° cuando la dependencia se recupere

# En ~30s en HALF_OPEN, probar√° una solicitud
# Si tiene √©xito, pasar√° a CLOSED autom√°ticamente
```

---

### Alert 4: Component Degraded (Componente Degradado)

**Nombre**: `KairosComponentDegraded`

**Regla (Prometheus)**:
```yaml
alert: KairosComponentDegraded
expr: kairos.health.status{component=~".+"} == 1
for: 3m
severity: warning
annotations:
  summary: "Componente degradado"
  description: |
    Componente: {{ $labels.component }}
    Estado: DEGRADED (funciona pero con limitaciones)
    Puede empeorar ‚Üí prepararse para fallback
  action: "Monitorear, estar listo para escalar o conmutar fallback"
```

**Cu√°ndo dispara**:
- Componente en estado DEGRADED por 3+ minutos

**Acciones**:
1. ‚úÖ Monitorear tasa de errores ‚Üí ¬ømejorando o empeorando?
2. ‚úÖ Si empeora, conmutar a fallback manualmente
3. ‚úÖ Si mejora, esperar a que se recupere completamente
4. ‚úÖ Post-mortem: ¬øpor qu√© degrad√≥?

---

### Alert 5: Component Unhealthy (Componente No Saludable)

**Nombre**: `KairosComponentUnhealthy`

**Regla (Prometheus)**:
```yaml
alert: KairosComponentUnhealthy
expr: kairos.health.status{component=~".+"} == 0
for: 1m
severity: critical
annotations:
  summary: "‚ö†Ô∏è COMPONENTE NO SALUDABLE - ACCI√ìN INMEDIATA"
  description: |
    Componente: {{ $labels.component }}
    Estado: UNHEALTHY (usando fallback, posible outage)
    
    Datos de tr√°fico:
    - Error rate: {{ $value_error_rate }}
    - Circuit breaker: OPEN
    - Fallback activo
  action: |
    üö® INVESTIGAR YA:
    1. ¬øEst√° la dependencia down?
    2. ¬øSe agot√≥ capacidad?
    3. ¬øProblema de red?
    ‚Üí Reiniciar, escalar, o conmutar a secondary
```

**Cu√°ndo dispara**:
- Componente en estado UNHEALTHY por 1+ minuto

**Acciones** (CR√çTICAS):
1. üö® Investigar causa ra√≠z inmediatamente
2. üö® Escalar infraestructura si es necesario
3. üö® Conmutar a servicio secondary si est√° disponible
4. üö® Comunicar a stakeholders si es outage p√∫blico

**Remediaci√≥n**:
```bash
# 1. Ver logs
kubectl logs <pod> --tail=200 | grep -i error

# 2. Ver recursos
kubectl describe pod <pod>

# 3. Escalar si es necesario
kubectl scale deployment llm-service --replicas=5

# 4. Si problem persiste, rollback cambio reciente
git revert <commit>

# 5. Conmutar fallback (si est√° configurado)
# Esto es manejo manual de la aplicaci√≥n
```

---

### Alert 6: Non-Recoverable Errors (Errores No Recuperables)

**Nombre**: `KairosNonRecoverableErrors`

**Regla (Prometheus)**:
```yaml
alert: KairosNonRecoverableErrors
expr: rate(kairos.errors.total{recoverable="false"}[5m]) > 1
for: 2m
severity: critical
annotations:
  summary: "Errores NO RECUPERABLES detectados"
  description: |
    Tasa: {{ $value }} non-recoverable errors/sec
    Estos NO se reintentar√°n, NO hay fallback
    
    Causas t√≠picas:
    - CodeUnauthorized: Token expirado, permisos incorrectos
    - CodeInvalidInput: Usuario pas√≥ datos inv√°lidos
    - CodeMemoryError: Bug en aplicaci√≥n
  action: "Revisar logs para identificar bug o config incorrecta"
```

**Cu√°ndo dispara**:
- M√°s de 1 error no-recuperable por segundo durante 2+ minutos

**Acciones**:
1. ‚úÖ Ver qu√© tipo de error no-recuperable: UNAUTHORIZED? INVALID_INPUT? MEMORY_ERROR?
2. ‚úÖ Investigar causa ra√≠z
3. ‚úÖ Esto indica un bug de aplicaci√≥n o misconfigraci√≥n

**Remediaci√≥n**:
```bash
# Ver qu√© espec√≠ficamente falla
kubectl logs <pod> | grep "non-recoverable\|UNAUTHORIZED\|INVALID_INPUT"

# Ejemplos:
# UNAUTHORIZED ‚Üí Revisar tokens en config
# INVALID_INPUT ‚Üí Validar entrada de usuario
# MEMORY_ERROR ‚Üí Investigar memory leak
```

---

## Ejemplos de Uso

### Ejemplo 1: Instrumentar un Servicio

```go
package main

import (
    "context"
    "log/slog"
    
    "github.com/jllopis/kairos/pkg/errors"
    "github.com/jllopis/kairos/pkg/resilience"
    "github.com/jllopis/kairos/pkg/telemetry"
    "go.opentelemetry.io/otel"
)

func main() {
    // 1. Inicializar telemetr√≠a
    shutdown, _ := telemetry.Init("my-service", "1.0.0")
    defer shutdown(context.Background())
    
    // 2. Crear m√©tricas
    metrics, _ := telemetry.NewErrorMetrics(context.Background())
    
    ctx := context.Background()
    tracer := otel.Tracer("my-service")
    
    // 3. En tu funci√≥n de negocio
    _, span := tracer.Start(ctx, "ProcessRequest")
    defer span.End()
    
    // 4. Llamar a servicio con reintentos
    retryConfig := resilience.DefaultRetryConfig().
        WithMaxAttempts(3).
        WithInitialDelay(100 * time.Millisecond)
    
    err := retryConfig.Do(ctx, func() error {
        return callLLM(ctx)
    })
    
    // 5. Registrar resultado
    if err != nil {
        // Error no recuperable
        metrics.RecordErrorMetric(ctx, err, "llm-service")
        telemetry.RecordError(span, err)
        slog.Error("LLM failed", "error", err)
        return err
    }
    
    // √âxito
    metrics.RecordRecovery(ctx, errors.CodeLLMError)
    return nil
}

func callLLM(ctx context.Context) error {
    // Si falla con error recuperable ‚Üí retry
    // Si falla con error no-recuperable ‚Üí RecordErrorMetric
    return errors.New(
        errors.CodeLLMError,
        "model overloaded",
        nil,
    ).WithRecoverable(true)
}
```

**Resultado en dashboards**:
- Counter `kairos.errors.total{error_code="LLM_ERROR", component="llm-service"}` incrementa en cada intento
- Counter `kairos.errors.recovered{error_code="LLM_ERROR"}` incrementa si retry funciona
- Tasa de recuperaci√≥n: (1 / 3 intentos) = 33% si falla despu√©s de 3 reintentos

---

### Ejemplo 2: Consultar M√©tricas en Grafana

**Dashboard: "Top Failing Components"**

```promql
# Muestra componentes con m√°s errores
topk(5, sum(rate(kairos.errors.total[5m])) by (component))
```

Resultado:
```
llm-service:       2.5 errors/sec
executor:          0.8 errors/sec
cache:             0.2 errors/sec
database:          0.1 errors/sec
```

**Action**: Investigar `llm-service` primero.

---

### Ejemplo 3: Interpretar una Cascada de Fallos

**Timeline**:
```
T=10:00:00 ‚Üí external-api.com sufre latencia (lento)
T=10:00:15 ‚Üí kairos.errors.total{error_code="TIMEOUT"} sube
T=10:00:30 ‚Üí kairos.circuitbreaker.state{component="api-client"} = 0 (OPEN)
T=10:00:35 ‚Üí kairos.health.status{component="llm-service"} = 1 (DEGRADED)
T=10:00:45 ‚Üí kairos.errors.rate{component="llm-service"} sube
T=10:01:00 ‚Üí AlertManager dispara KairosCircuitBreakerOpen + KairosHighErrorRate
T=10:02:00 ‚Üí external-api.com recuperada
T=10:02:15 ‚Üí kairos.circuitbreaker.state retorna a 2 (CLOSED)
T=10:02:30 ‚Üí kairos.health.status vuelve a 2 (HEALTHY)
```

**Insight**: Un timeout en un servicio externo cascade√≥ a trav√©s del sistema. Circuit breaker + health checks evit√≥ outage total.

---

## Integraci√≥n con Backends

### Datadog

```yaml
# datadog-agent-helm-values.yaml
datadog:
  apiKey: <API_KEY>
  otlp:
    enabled: true
    receiver:
      protocols:
        grpc:
          enabled: true
          port: 4317
        http:
          enabled: true
          port: 4318
```

**Configurar Kairos**:
```go
metrics, _ := telemetry.NewErrorMetrics(ctx)
shutdown, _ := telemetry.InitWithConfig(
    "my-service", "1.0.0",
    telemetry.Config{
        Exporter:     "otlp",
        OTLPEndpoint: "datadog-agent:4317",
    },
)
```

### New Relic

```go
shutdown, _ := telemetry.InitWithConfig(
    "my-service", "1.0.0",
    telemetry.Config{
        Exporter:     "otlp",
        OTLPEndpoint: "otlp.nr-data.net:4317",
    },
)
```

Luego configurar API key en variable de entorno `OTEL_EXPORTER_OTLP_HEADERS`.

### Prometheus + Grafana (On-Premise)

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'kairos'
    static_configs:
      - targets: ['localhost:8888']  # OTEL Prometheus exporter
```

**Deploy**:
```bash
docker run -p 9090:9090 prom/prometheus --config.file=prometheus.yml
docker run -p 3000:3000 grafana/grafana
```

---

## SLOs y Recomendaciones

### SLO 1: Error Rate

**Meta**: < 5 errores/min (99.9% availability a 1M req/min)

```promql
# Alertar si se excede durante 10 min
avg(rate(kairos.errors.total[5m])) over (10m) > 5
```

**Acciones**:
- Escalar autom√°ticamente si es sobrecarga
- Investigar si hay bug nuevo

---

### SLO 2: Recovery Rate

**Meta**: > 80% de errores se recuperan

```promql
# Alertar si recovery rate cae
(
  avg(rate(kairos.errors.recovered[5m])) over (10m)
  /
  avg(rate(kairos.errors.total[5m])) over (10m)
) < 0.8
```

**Acciones**:
- Aumentar MaxAttempts en RetryConfig
- Activar fallback strategies
- Revisar qu√© errores son non-recoverable

---

### SLO 3: Component Health

**Meta**: Todos los componentes HEALTHY >= 95% del tiempo

```promql
# Calcular uptime por componente
(
  sum(increase(kairos.health.status{component=~".+", status="HEALTHY"}[1d]))
  /
  sum(increase(kairos.health.status{component=~".+"}[1d]))
) * 100
```

---

### Recomendaciones Generales

1. **Baselines**: Establece baselines para tu servicio (error rate normal, recovery rate esperada)
2. **Tunning**: Ajusta thresholds de alertas basado en tu SLO
3. **Runbooks**: Para cada alerta, tener runbook de remediaci√≥n
4. **Correlaci√≥n**: Buscar patrones ‚Üí ¬øerror A siempre dispara error B?
5. **Costo**: Monitorea CodeRateLimit para optimizar capacity planning

---

## Referencias

- [Manejo de Errores en Kairos](ERROR_HANDLING.md)
- [Documentaci√≥n OTEL](https://opentelemetry.io/)
- [Prometheus Query Language](https://prometheus.io/docs/prometheus/latest/querying/basics/)
- [Grafana Dashboard Best Practices](https://grafana.com/docs/grafana/latest/dashboards/)
